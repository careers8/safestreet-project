{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CityIQ Project SafeStreet\n",
    "\n",
    "<br> ORGANIZATION: IGNITE | SCALE | SAN DIEGO 2019 Smart Cities Hackathon and Innovation Program\n",
    "\n",
    "<br> PURPOSE: The goal of the CityIQ Project SafeStreet is to analyze CityIQ sensor data in order to determine what features predict traffic-related deaths on San Diego County roadways.\n",
    "\n",
    "<br> <b>UPDATE: The City of San Diego has ended its contract with the CityIQ data provider ( Ubicquia, Inc. ), and data is no longer accessible through the API. This notebook therefore exists as a CityIQ data analysis proof-of-concept.</b>\n",
    "\n",
    "<br> CityIQ API Documentation Sources:\n",
    "<br> https://docs.cityiq.io/Default.htm\n",
    "<br> https://github.com/CityIQ\n",
    "<br> https://github.com/Ubicquia/CityIQ\n",
    "<br> https://github.com/opensandiego/sd-smart-streetlight-tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# CityIQ API requests.\n",
    "import requests\n",
    "\n",
    "# Epoch calculations.\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Visualizations imports.\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "from folium.plugins import HeatMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definitions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTTP GET Request\n",
    "Meta-/data acquisition is performed by GET requests to the CityIQ REST APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_request(url, headers, params):    \n",
    "    payload = \"\"   \n",
    "    response = requests.request(\"GET\", url, data=payload, headers=headers, params=params)\n",
    "    return(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Data by assetUid\n",
    "The purpose of this function is to perform GET requests for each asset from a list of assetUids and return the contents as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_by_assetUid(auid, h, p):\n",
    "    arr = []\n",
    "    for i in auid:\n",
    "        uri = \"https://sandiego.cityiq.io/api/v2/event/assets/\" + i + \"/events\"\n",
    "        get = get_request(uri, h, p)\n",
    "        if get['metaData']['totalRecords'] > 0:\n",
    "            arr.append(get['content'])\n",
    "    return(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flatten List\n",
    "The purpose of this function is to flatten the list that is generated by the data_by_assetUid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(arr):\n",
    "    list_o_dics = []\n",
    "    for l in arr:\n",
    "        for dic in l:\n",
    "            list_o_dics.append(dic)\n",
    "    return(list_o_dics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Data by Date\n",
    "\n",
    "The purpose of the following functions is to create a list of dates from which to retrieve asset data. Specify how many days back you want to go ( from yesterday's date )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def millis(dt):\n",
    "    epoch = datetime.datetime.utcfromtimestamp(0)\n",
    "    return(int((dt - epoch).total_seconds() * 1000.0))\n",
    "\n",
    "def get_date_list(days_back):    \n",
    "    base = datetime.datetime.today() - datetime.timedelta(1)\n",
    "    dates = [(base - datetime.timedelta(days=x)).strftime('%m/%d/%Y') for x in range(0, days_back)]\n",
    "    epoch_dates = [(millis(datetime.datetime.strptime(x + ' 00:00:00','%m/%d/%Y %H:%M:%S')),millis(datetime.datetime.strptime(x + ' 23:59:59','%m/%d/%Y %H:%M:%S'))) for x in dates]\n",
    "    return(epoch_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# locations.csv\n",
    "This dataset contains extracts of the assets and locations from the San Diego CityIQ system. Refer to the CityIQ developer documentation for details about these data records. The data are extracted using the cityiq Python package. See the ExtractAssets.ipynb notebook for the extract process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://data.sandiegodata.org/dataset/sandiego-gov-cityiq_objects\n",
    "# Define path to data.\n",
    "loc_sdrdl_fp = \"./locations.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pd_collisions_datasd.csv\n",
    "This dataset contains traffic collision reports within the City of San Diego. Generally, a report is not taken for property damage-only collisions that do not involve hit & run or DUI. The California Highway Patrol is responsible for handling collisions occurring on the freeway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to data.\n",
    "collisions_fp = \"./pd_collisions_datasd.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pedestrians.csv\n",
    "This dataset contains counts of pedestrian in walkways, in San Diego, from August 2018 through March 2019, aggregated to 15 minutes. Use the CityIQ Assets and Locations dataset for the geographic positions of the walkways.\n",
    "\n",
    "The original data also has values for counts in each direction of the walkway, and the speed. Unfortunately, the geographic data for the walkways -- lines -- are usually wrong, with many walkways being incorrectly long and poorly positioned. The result is that many of the speed values are much too high, so for this dataset the speed and direction values are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://data.sandiegodata.org/dataset/cityiq-pedestrians\n",
    "# Define path to data.\n",
    "ped_sdrdl_fp = \"./pedestrians.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CityIQ API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URLs\n",
    "The data acquisition from the CityIQ REST APIs is performed by GET requests to the following URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = { 'uaa': \"https://auth.aa.cityiq.io/oauth/token\", # CityIQ UAA.\n",
    "         'meta': \"https://sandiego.cityiq.io/api/v2/metadata/assets/search\", # CityIQ Metadata Service.\n",
    "         'event_by_bbox': \"https://sandiego.cityiq.io/api/v2/event/locations/events\", # CityIQ Event Service ( retrieve by bbox ).\n",
    "         'event_by_auid': \"https://sandiego.cityiq.io/api/v2/event/assets/\" # CityIQ Event Service ( retrieve by assetUid ).\n",
    "         # ,'event_by_luid': \"https://sandiego.cityiq.io/api/v2/event/locations/\" # CityIQ Event Service ( retrieve by locationUid ).\n",
    "       }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postman-Tokens\n",
    "The GET request header requires a parameter called \"Postman-Token\", which varies by type of asset/event request. We do not have access to the Metrology or Media zones, so cannot access their data ( these tokens are commented out )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptokens = { 'uaa': \"8c5854aa-9b88-482f-978a-77916bfd9792\", # CityIQ UAA.\n",
    "            'meta': \"f51ab022-3288-4fbe-a7d2-5781fe449378\", # CityIQ Metadata Service.\n",
    "            'pkin_auid': \"1284f57d-1a27-4b7d-947b-a53ece2a01b8\", # CityIQ Parking Event Service ( retrieve by assetId ).\n",
    "            'pkin_bbox': \"01d2538e-beeb-4498-964f-6f97a9538d5c\", # CityIQ Parking Event Service ( retrieve by bbox ).\n",
    "            'pkin_luid': \"a79e2341-0b9e-4dc1-8d10-29f8e28e3c99\", # CityIQ Parking Event Service ( retrieve by locationId ).\n",
    "            'pkout_auid': \"687c56b4-b1fe-47a2-8312-58f92e4276aa\", # CityIQ Parking Event Service ( retrieve by assetId ).\n",
    "            'pkout_bbox': \"114157a1-7f40-495b-a1f3-716c9505d8fb\", # CityIQ Parking Event Service ( retrieve by bbox ).\n",
    "            'pkout_luid': \"cd48f6dc-e034-41a2-9aa2-fbbad13e6bfc\", # CityIQ Parking Event Service ( retrieve by locationId ).\n",
    "            'pedevt_auid': \"833733ae-b5b6-4168-a9a9-610f5b0fe6cb\", # CityIQ Pedestrian Event Service ( retreive by assetId ).          \n",
    "            'pedevt_bbox': \"f51ab022-3288-4fbe-a7d2-5781fe449378\", # CityIQ Pedestrian Event Service ( retrieve by bbox ).\n",
    "            'pedevt_luid': \"22b8e9c1-be36-4aef-9eb8-4a37f32f8679\", # CityIQ Pedestrian Event Service ( retrieve by locationId ).\n",
    "            'tfevt_auid': \"26685807-3fbb-43ce-b200-e0f5ebb53073\", # CityIQ Traffic Event Service ( retrieve by assetId ).\n",
    "            'tfevt_bbox': \"4ed0a27c-c91e-4baa-93b1-3a7434c25e1b\", # CityIQ Traffic Event Service ( retrieve by bbox ).\n",
    "            'tfevt_luid': \"8f62aa35-2223-4351-a462-9b1a9f2a5913\", # CityIQ Pedestrian Event Service ( retrieve by locationId ).\n",
    "            'humidity': \"a7299bd9-5a78-422a-ba3d-f6bfb142b74a\", # CityIQ Environmental Event Service ( retrieve by assetId ).\n",
    "            'orientation': \"308e24be-78fc-40c8-9eb7-a37dab020494\", # CityIQ Environmental Event Service ( retrieve by assetId ).\n",
    "            'pressure': \"f9b81deb-cd11-47ce-be04-13a1dc858f63\", # CityIQ Environmental Event Service ( retrieve by assetId ).\n",
    "            'temperature': \"82bd2ae9-c991-4a80-abd2-7e54abcbcd79\", # CityIQ Environmental Event Service ( retrieve by assetId ).\n",
    "            'humidity': \"a7299bd9-5a78-422a-ba3d-f6bfb142b74a\" # CityIQ Environmental Event Service ( retrieve by assetId ).\n",
    "            # ,'energy_alert': \"a9f2834c-8cdf-4ee0-a73c-6aa3614db52f\",  # CityIQ Metrology Event Service ( retrieve by assetId ).\n",
    "            # ,'energy_timeseries': \"f71bae62-2f6c-48dd-b3fd-4f6b58f6823b\", # CityIQ Metrology Event Service ( retrieve by assetId).\n",
    "            # ,'metrology': \"8e876aef-25f9-46e9-bfb3-96ff628023bd\" # CityIQ Metrology Event Service ( retrieve by assetUid).\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predix Zone IDs\n",
    "The GET request header requires Predix zone IDs, which define separate data streams ( ENVIRONMENTAL, ENERGY-METERING, MEDIA, PARKING, PEDESTRIAN, TRAFFIC ). The zone IDs indicate the different sub-APIs that we have access to, which allow us to pull their corresponding event type data. See the access token 'scope' value for which data streams we have access to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_ids = { 'environmental': \"SD-IE-ENVIRONMENTAL\",\n",
    "             'parking': \"SD-IE-PARKING\",\n",
    "             'pedestrian': \"SD-IE-PEDESTRIAN\",\n",
    "             'traffic': \"SD-IE-TRAFFIC\"\n",
    "             # ,'audio': \"SD-ID-AUDIO\",\n",
    "             # ,'image': \"SD-IE-IMAGE\",\n",
    "             # ,'metrology': \"SD-IE-METROLOGY\",\n",
    "             # ,'video': \"SD-IE-VIDEO\"\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boundary Boxes\n",
    "Meta-/data can be filtered using the boundary box parameter, which is defined by northwest and southeast latitude and longitude coordinates, and represents a geographic area. For example, this parameter could be used to collect asset metadata ( e.g. assetUids ) for assets along specific Vision Zero corridors ( may require sequential queries to obtain entire corridor ).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes = { 'el_cajon_blvd': \"33.077762:-117.663817,32.559574:-116.584410\",\n",
    "           'downtown_san_diego': \"32.718987:-117.174244,32.707356:-117.154850\"\n",
    "           # ,'4000_4100_university_ave': \"32.749516:-117.104581,32.749862:-117.109569\",\n",
    "           # ,'university_ave': \"32.747555:-117.073596,32.749721:-117.173804\",\n",
    "           # ,'petco_park': \"32.709496:-117.159329,32.705434:-117.154362\"\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timestamps\n",
    "Event data can be pulled within a specific epoch, defined by timestamps bounding the interval ( in Unix time )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define epoch with timestamps in the following format: YEAR-MONTH-DAY HOUR:MINUTE:SECOND MICROSECONDS.\n",
    "START = \"2019-05-1 00:00:00\" \n",
    "END = \"2019-05-1 23:59:59\"\n",
    "\n",
    "# Convert into tuples for time module.\n",
    "start_tuple = time.strptime(START, '%Y-%m-%d %H:%M:%S')\n",
    "end_tuple = time.strptime(END, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Define timestamps in milliseconds ( to nearest second ).\n",
    "STARTTS = int(1000*time.mktime(start_tuple))\n",
    "ENDTS = int(1000*time.mktime(end_tuple))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asset Types\n",
    "Types of assets ( sensors ) available. Each asset can only be of a single type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assets = { 'camera': \"CAMERA\", \n",
    "           'environmental': \"ENV_SENSOR\", \n",
    "           'em': \"EM_SENSOR\",\n",
    "           'microphone': \"MIC\", \n",
    "           'node': 'NODE'\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Types\n",
    "Types of events ( collections of data from sensors ) available. Each asset may have multiple event types ( e.g. CAMERA asset that collects PEDEVT and TFEVT events )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = { 'humidity': \"HUMIDITY\",\n",
    "           'orientation': \"ORIENTATION\",\n",
    "           'pedestrian': \"PEDEVT\",\n",
    "           'parking_in': \"PKIN\",\n",
    "           'parking_out': \"PKOUT\",\n",
    "           'pressure': \"PRESSURE\",\n",
    "           'temperature': \"TEMPERATURE\",\n",
    "           'traffic': \"TFEVT\"\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Location Types\n",
    "Types of software-defined boundaries that the camera assets use to detect events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = { 'parking': \"PARKING_ZONE\",\n",
    "              'pedestrian': \"WALKWAY\",\n",
    "              'traffic': \"TRAFFIC_LANE\"\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CityIQ Access Token\n",
    "\n",
    "Let's first grab the access token, so that we can make the API calls.\n",
    "\n",
    "<b> Unfortunately, due to the inavailability of the public client id and client secret credentials, the Postman-Token is no longer valid, and thus the API is no longer accessable through this notebook ( please see the UPDATE section at the head of this notebook for further information ). </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define header dictionary.\n",
    "uaa_headers = { 'Authorization': \"Basic UHVibGljQWNjZXNzOnVWZWVNdWl1ZTRrPQ==\",\n",
    "                'cache-control': \"no-cache\",\n",
    "                'Postman-Token': ptokens['uaa']\n",
    "              }\n",
    "\n",
    "# Define parameter dictionary.    \n",
    "uaa_params = {\"grant_type\":\"client_credentials\"}    \n",
    "\n",
    "# Return access token and other information as dictionary.\n",
    "atoken = get_request(urls['uaa'], uaa_headers, uaa_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CityIQ Asset Metadata\n",
    "\n",
    "Next let's grab the asset metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define header dictionary.\n",
    "meta_headers = { 'Authorization': atoken['token_type'] + atoken['access_token'],\n",
    "                 'Predix-Zone-Id': zone_ids['traffic'], # Any zone ID will work for metadata.\n",
    "                 'cache-control': \"no-cache\",\n",
    "                 'Postman-Token': ptokens['meta']\n",
    "               }\n",
    "\n",
    "# Define parameter dictionary.\n",
    "ATYPE = \"assetType:\" + assets['camera'] # Filter response by asset type.\n",
    "Q = ATYPE # Query using a filter ( assetType, eventTypes, mediaType ).\n",
    "PAGE = 0 # Indicates page number ( default = 0 ).\n",
    "SIZE = 30000 # Maximum number of records to return per page ( default = 20 ).\n",
    "meta_params = { #\"bbox\": bboxes['downtown_san_diego'], # Comment line to return all assets, irrespective of coordinates.\n",
    "                #\"q\": Q, # Comment line to return all asset types.\n",
    "                \"page\": PAGE,\n",
    "                \"size\": SIZE,\n",
    "              }\n",
    "\n",
    "# Return asset metadata and other information as dictionary.\n",
    "all_meta_dict = get_request(urls['meta'], meta_headers, meta_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only Pedestrian Event Asset Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define header dictionary.\n",
    "meta_headers = { 'Authorization': atoken['token_type'] + atoken['access_token'],\n",
    "                 'Predix-Zone-Id': zone_ids['pedestrian'], # Any zone ID will work for metadata.\n",
    "                 'cache-control': \"no-cache\",\n",
    "                 'Postman-Token': ptokens['meta']\n",
    "               }\n",
    "\n",
    "# Define parameter dictionary.\n",
    "ETYPE = \"eventTypes:\" + events['pedestrian'] # Filter response by event type.\n",
    "Q = ETYPE # Query using a filter ( assetType, eventTypes, mediaType ).\n",
    "PAGE = 0 # Indicates page number ( default = 0 ).\n",
    "SIZE = 30000 # Maximum number of records to return per page ( default = 20 ).\n",
    "meta_params = { #\"bbox\": bboxes['downtown_san_diego'], # Comment line to return all assets, irrespective of coordinates.\n",
    "                \"q\": Q, # Comment line to return all asset types.\n",
    "                \"page\": PAGE,\n",
    "                \"size\": SIZE,\n",
    "              }\n",
    "\n",
    "# Return asset metadata and other information as dictionary.\n",
    "ped_meta_dict = get_request(urls['meta'], meta_headers, meta_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only Traffic Event Asset Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define header dictionary.\n",
    "meta_headers = { 'Authorization': atoken['token_type'] + atoken['access_token'],\n",
    "                 'Predix-Zone-Id': zone_ids['traffic'], # Any zone ID will work for metadata.\n",
    "                 'cache-control': \"no-cache\",\n",
    "                 'Postman-Token': ptokens['meta']\n",
    "               }\n",
    "\n",
    "# Define parameter dictionary.\n",
    "ETYPE = \"eventTypes:\" + events['traffic'] # Filter response by event type.\n",
    "Q = ETYPE # Query using a filter ( assetType, eventTypes, mediaType ).\n",
    "PAGE = 0 # Indicates page number ( default = 0 ).\n",
    "SIZE = 30000 # Maximum number of records to return per page ( default = 20 ).\n",
    "meta_params = { #\"bbox\": bboxes['downtown_san_diego'], # Comment line to return all assets, irrespective of coordinates.\n",
    "                \"q\": Q, # Comment line to return all asset types.\n",
    "                \"page\": PAGE,\n",
    "                \"size\": SIZE,\n",
    "              }\n",
    "\n",
    "# Return asset metadata and other information as dictionary.\n",
    "traf_meta_dict = get_request(urls['meta'], meta_headers, meta_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only Humidity Event Asset Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define header dictionary.\n",
    "meta_headers = { 'Authorization': atoken['token_type'] + atoken['access_token'],\n",
    "                 'Predix-Zone-Id': zone_ids['environmental'], # Any zone ID will work for metadata.\n",
    "                 'cache-control': \"no-cache\",\n",
    "                 'Postman-Token': ptokens['meta']\n",
    "               }\n",
    "\n",
    "# Define parameter dictionary.\n",
    "ETYPE = \"eventTypes:\" + events['humidity'] # Filter response by event type.\n",
    "Q = ETYPE # Query using a filter ( assetType, eventTypes, mediaType ).\n",
    "PAGE = 0 # Indicates page number ( default = 0 ).\n",
    "SIZE = 30000 # Maximum number of records to return per page ( default = 20 ).\n",
    "meta_params = { #\"bbox\": bboxes['downtown_san_diego'], # Comment line to return all assets, irrespective of coordinates.\n",
    "                \"q\": Q, # Comment line to return all asset types.\n",
    "                \"page\": PAGE,\n",
    "                \"size\": SIZE,\n",
    "              }\n",
    "\n",
    "# Return asset metadata and other information as dictionary.\n",
    "hum_meta_dict = get_request(urls['meta'], meta_headers, meta_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only Orientation Event Asset Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define header dictionary.\n",
    "meta_headers = { 'Authorization': atoken['token_type'] + atoken['access_token'],\n",
    "                 'Predix-Zone-Id': zone_ids['environmental'], # Any zone ID will work for metadata.\n",
    "                 'cache-control': \"no-cache\",\n",
    "                 'Postman-Token': ptokens['meta']\n",
    "               }\n",
    "\n",
    "# Define parameter dictionary.\n",
    "ETYPE = \"eventTypes:\" + events['orientation'] # Filter response by event type.\n",
    "Q = ETYPE # Query using a filter ( assetType, eventTypes, mediaType ).\n",
    "PAGE = 0 # Indicates page number ( default = 0 ).\n",
    "SIZE = 30000 # Maximum number of records to return per page ( default = 20 ).\n",
    "meta_params = { #\"bbox\": bboxes['downtown_san_diego'], # Comment line to return all assets, irrespective of coordinates.\n",
    "                \"q\": Q, # Comment line to return all asset types.\n",
    "                \"page\": PAGE,\n",
    "                \"size\": SIZE,\n",
    "              }\n",
    "\n",
    "# Return asset metadata and other information as dictionary.\n",
    "ori_meta_dict = get_request(urls['meta'], meta_headers, meta_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only Pressure Event Asset Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define header dictionary.\n",
    "meta_headers = { 'Authorization': atoken['token_type'] + atoken['access_token'],\n",
    "                 'Predix-Zone-Id': zone_ids['environmental'], # Any zone ID will work for metadata.\n",
    "                 'cache-control': \"no-cache\",\n",
    "                 'Postman-Token': ptokens['meta']\n",
    "               }\n",
    "\n",
    "# Define parameter dictionary.\n",
    "ETYPE = \"eventTypes:\" + events['pressure'] # Filter response by event type.\n",
    "Q = ETYPE # Query using a filter ( assetType, eventTypes, mediaType ).\n",
    "PAGE = 0 # Indicates page number ( default = 0 ).\n",
    "SIZE = 30000 # Maximum number of records to return per page ( default = 20 ).\n",
    "meta_params = { #\"bbox\": bboxes['downtown_san_diego'], # Comment line to return all assets, irrespective of coordinates.\n",
    "                \"q\": Q, # Comment line to return all asset types.\n",
    "                \"page\": PAGE,\n",
    "                \"size\": SIZE,\n",
    "              }\n",
    "\n",
    "# Return asset metadata and other information as dictionary.\n",
    "pres_meta_dict = get_request(urls['meta'], meta_headers, meta_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only Temperature Event Asset Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define header dictionary.\n",
    "meta_headers = { 'Authorization': atoken['token_type'] + atoken['access_token'],\n",
    "                 'Predix-Zone-Id': zone_ids['environmental'], # Any zone ID will work for metadata.\n",
    "                 'cache-control': \"no-cache\",\n",
    "                 'Postman-Token': ptokens['meta']\n",
    "               }\n",
    "\n",
    "# Define parameter dictionary.\n",
    "ETYPE = \"eventTypes:\" + events['temperature'] # Filter response by event type.\n",
    "Q = ETYPE # Query using a filter ( assetType, eventTypes, mediaType ).\n",
    "PAGE = 0 # Indicates page number ( default = 0 ).\n",
    "SIZE = 30000 # Maximum number of records to return per page ( default = 20 ).\n",
    "meta_params = { #\"bbox\": bboxes['downtown_san_diego'], # Comment line to return all assets, irrespective of coordinates.\n",
    "                \"q\": Q, # Comment line to return all asset types.\n",
    "                \"page\": PAGE,\n",
    "                \"size\": SIZE,\n",
    "              }\n",
    "\n",
    "# Return asset metadata and other information as dictionary.\n",
    "temp_meta_dict = get_request(urls['meta'], meta_headers, meta_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Event Type Heat Maps\n",
    "Generate heat maps showing the density of nodes with assets actively recording the specified event type. This will enable a visualization of the geographic distribution of the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the all_meta_dict dictionary into a DataFrame.\n",
    "df = pd.DataFrame(all_meta_dict['content'])\n",
    "\n",
    "# The number of unique parentAssetUids is the number of nodes ( 3013 ) + 1 ( nodes themselves have a parentAssetUid of None)\n",
    "display(len(df[~df.parentAssetUid.isna()].parentAssetUid.unique()))\n",
    "\n",
    "# Verify that every sensor belongs to one of the nodes.\n",
    "display((len(df[df.assetType == \"NODE\"].assetUid.unique()) == len(df[df.assetType == \"NODE\"].assetUid))) # Verifies that each node has a unique assetUid ( equivalent to parentAssetUid for sensors ).\n",
    "display(df[df.assetType != \"NODE\"].parentAssetUid.isna().unique()) # Verifies that none of the sensors lack a parentAssetUid.\n",
    "display(df[~df.parentAssetUid.isna()].parentAssetUid.isin(df[df.assetType == \"NODE\"].assetUid.unique()).unique()) # Verifies that each sensor's parentAssetUid corresponds an existing node.\n",
    "\n",
    "# Generate a dictionary containing of the all event types each node measures.\n",
    "evts = { auid: list(sorted(set(flatten(df[(df.parentAssetUid == auid) & (~df.eventTypes.isna())].eventTypes)))) \\\n",
    "         for auid in df[df.assetType == \"NODE\"].assetUid.unique() \n",
    "       }\n",
    "\n",
    "# Generate a dictionary containing the coordinates of each node.\n",
    "coors = { auid: str(df[df.assetUid == auid].coordinates.values) \\\n",
    "         for auid in df[df.assetType == \"NODE\"].assetUid.unique() \n",
    "        }\n",
    "\n",
    "# Convert dictionaries into DataFrames.\n",
    "evts_df = pd.DataFrame({'node_assetUid': list(evts.keys()), 'eventTypes': list(evts.values())})\n",
    "coors_df = pd.DataFrame({'node_assetUid': list(coors.keys()), 'coordinates': list(coors.values())})\n",
    "\n",
    "# Verify that node_assetUids match up by index.\n",
    "display(np.all((evts_df.node_assetUid.values == coors_df.node_assetUid.values) == True))\n",
    "\n",
    "# Create a DataFrame containing the coordinates and eventTypes for each node.\n",
    "coordinates = coors_df.coordinates.str.strip(\"[]''\").str.split(':')\n",
    "all_df = pd.DataFrame()\n",
    "all_df[\"latitude\"] = [float(coordinates[i][0]) for i in coordinates.index]\n",
    "all_df[\"longitude\"] = [float(coordinates[i][1]) for i in coordinates.index]\n",
    "all_df[\"eventTypes\"] = list(evts_df.eventTypes.values)\n",
    "\n",
    "# One-hot-encoding of eventTypes. ENERGY_TIMESERIES ends up as two separate Series, depending on its order in the original lists.\n",
    "one_df = all_df.eventTypes.apply(', '.join).str.get_dummies(sep=\", \")\n",
    "one_df = pd.concat([all_df, one_df], axis=1)\n",
    "\n",
    "# Define mean latitude and longitude to start map at.\n",
    "lat_mean = one_df.describe().at['mean','latitude']\n",
    "long_mean = one_df.describe().at['mean','longitude']\n",
    "m = folium.Map([lat_mean, long_mean],\n",
    "                      zoom_start=10)\n",
    "\n",
    "# Apply the heat map to the Folium map, for the given event type ( e.g. PRESSURE ).\n",
    "HeatMap(one_df[['latitude', 'longitude', 'PRESSURE']].values, min_opacity =0.4).add_to(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CityIQ Asset Event Data\n",
    "\n",
    "Next let's grab the event data. Retrieving from the REST API; a WebSocket may be used in the future to obtain near-real-time data. We will do so separately, as they require distinct zone ID, eventType and locationType parameters ( different event types query different sub-APIs )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a test case, we will collect all of the event data for each asset on a single node, which specified below ( this node has assets that collect pedestrian, traffic, and environmental events ). We will accomplish this by requesting event data by the assetUid of each asset. To grab these assetUids, we will first convert the metadata dictionaries to pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parentAssetUid to extract events from a single node.\n",
    "pauid = '08cbccff-cdd0-404f-af62-0346a4480d5c'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dictionaries to DataFrames.\n",
    "all_meta_df = pd.DataFrame(all_meta_dict['content'])\n",
    "ped_meta_df = pd.DataFrame(ped_meta_dict['content'])\n",
    "traf_meta_df = pd.DataFrame(traf_meta_dict['content'])\n",
    "hum_meta_df = pd.DataFrame(hum_meta_dict['content'])\n",
    "ori_meta_df = pd.DataFrame(ori_meta_dict['content'])\n",
    "pres_meta_df = pd.DataFrame(pres_meta_dict['content'])\n",
    "temp_meta_df = pd.DataFrame(temp_meta_dict['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pedestrian Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define header dictionary.\n",
    "pedevt_headers = { 'Authorization': atoken['token_type'] + atoken['access_token'],\n",
    "                   'Predix-Zone-Id': zone_ids['pedestrian'],\n",
    "                   'cache-control': \"no-cache\",\n",
    "                   'Postman-Token': ptokens['pedevt_auid']\n",
    "                 }\n",
    "\n",
    "# Define parameter dictionary.\n",
    "SIZE = 100 # Maximum number of records to return per page ( default = 20 ).\n",
    "pedevt_params = { \"eventType\": events['pedestrian'],\n",
    "                  #\"bbox\": bboxes['downtown_san_diego'],\n",
    "                  #\"locationType\": \"WALKWAY\",#locations['pedestrian'],\n",
    "                  \"startTime\": STARTTS,\n",
    "                  \"endTime\": ENDTS,\n",
    "                  \"pageSize\": SIZE\n",
    "                }\n",
    "\n",
    "# Return event data by assetUid.\n",
    "pedevt_arr = data_by_assetUid(ped_meta_df[ped_meta_df['parentAssetUid'] == pauid].assetUid.values, pedevt_headers, pedevt_params)\n",
    "pedevt_arr = flatten(pedevt_arr) # Extract the inner list of dictionaries from the list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define header dictionary.\n",
    "tfevt_headers = { 'Authorization': atoken['token_type'] + atoken['access_token'],\n",
    "                  'Predix-Zone-Id': zone_ids['traffic'],\n",
    "                  'cache-control': \"no-cache\",\n",
    "                  'Postman-Token': ptokens['tfevt_auid']\n",
    "                }\n",
    "\n",
    "# Define parameter dictionary.\n",
    "SIZE = 3000 # Maximum number of records to return per page ( default = 20 ).\n",
    "tfevt_params = { \"eventType\": events['traffic'],\n",
    "                 #\"bbox\": bboxes['downtown_san_diego'],\n",
    "                 #\"locationType\": locations['traffic'],\n",
    "                 \"startTime\": STARTTS,\n",
    "                 \"endTime\": ENDTS,\n",
    "                 \"pageSize\": SIZE\n",
    "               }\n",
    "\n",
    "# Return event data by assetUid.\n",
    "tfevt_arr = data_by_assetUid(traf_meta_df[traf_meta_df['parentAssetUid'] == pauid].assetUid.values, tfevt_headers, tfevt_params)\n",
    "tfevt_arr = flatten(tfevt_arr) # Extract the inner list of dictionaries from the list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Humidity Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define header dictionary.\n",
    "hum_headers = { 'Authorization': atoken['token_type'] + atoken['access_token'],\n",
    "                'Predix-Zone-Id': zone_ids['environmental'],\n",
    "                'cache-control': \"no-cache\",\n",
    "                'Postman-Token': ptokens['humidity']\n",
    "              }\n",
    "\n",
    "# Define parameter dictionary.\n",
    "SIZE = 1000 # Maximum number of records to return per page ( default = 20 ).\n",
    "hum_params = { \"eventType\": events['humidity'],\n",
    "               #\"bbox\": bboxes['downtown_san_diego'],\n",
    "               #\"locationType\": LTYPES['traffic'],\n",
    "               \"startTime\": STARTTS,\n",
    "               \"endTime\": ENDTS,\n",
    "               \"pageSize\": SIZE\n",
    "             }\n",
    "\n",
    "# Return event data by assetUid.\n",
    "hum_arr = data_by_assetUid(hum_meta_df[hum_meta_df['parentAssetUid'] == pauid].assetUid.values, hum_headers, hum_params)\n",
    "hum_arr = flatten(hum_arr) # Extract the inner list of dictionaries from the list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orientation Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define header dictionary.\n",
    "ori_headers = { 'Authorization': atoken['token_type'] + atoken['access_token'],\n",
    "                'Predix-Zone-Id': zone_ids['environmental'],\n",
    "                'cache-control': \"no-cache\",\n",
    "                'Postman-Token': ptokens['orientation']\n",
    "              }\n",
    "\n",
    "# Define parameter dictionary.\n",
    "SIZE = 1000 # Maximum number of records to return per page ( default = 20 ).\n",
    "ori_params = { \"eventType\": events['orientation'],\n",
    "               #\"bbox\": bboxes['downtown_san_diego'],\n",
    "               \"locationType\": locations['traffic'],\n",
    "               \"startTime\": STARTTS,\n",
    "               \"endTime\": ENDTS,\n",
    "               \"pageSize\": SIZE\n",
    "               }\n",
    "\n",
    "# Return event data by assetUid.\n",
    "ori_arr = data_by_assetUid(ori_meta_df[ori_meta_df['parentAssetUid'] == pauid].assetUid.values, ori_headers, ori_params)\n",
    "ori_arr = flatten(ori_arr)  # Extract the inner list of dictionaries from the list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pressure Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define header dictionary.\n",
    "pres_headers = { 'Authorization': atoken['token_type'] + atoken['access_token'],\n",
    "                 'Predix-Zone-Id': zone_ids['environmental'],\n",
    "                 'cache-control': \"no-cache\",\n",
    "                 'Postman-Token': ptokens['humidity']\n",
    "               }\n",
    "\n",
    "# Define parameter dictionary.\n",
    "SIZE = 1000 # Maximum number of records to return per page ( default = 20 ).\n",
    "pres_params = { \"eventType\": events['pressure'],\n",
    "                #\"bbox\": bboxes['downtown_san_diego'],\n",
    "                #\"locationType\": locations['traffic'],\n",
    "                \"startTime\": STARTTS,\n",
    "                \"endTime\": ENDTS,\n",
    "                \"pageSize\": SIZE\n",
    "              }\n",
    "\n",
    "# Return event data by assetUid.\n",
    "pres_arr = data_by_assetUid(pres_meta_df[pres_meta_df['parentAssetUid'] == pauid].assetUid.values, pres_headers, pres_params)\n",
    "pres_arr = flatten(pres_arr) # Extract the inner list of dictionaries from the list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temperature Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define header dictionary.\n",
    "temp_headers = { 'Authorization': atoken['token_type'] + atoken['access_token'],\n",
    "                 'Predix-Zone-Id': zone_ids['environmental'],\n",
    "                 'cache-control': \"no-cache\",\n",
    "                 'Postman-Token': ptokens['temperature']\n",
    "               }\n",
    "\n",
    "# Define parameter dictionary.\n",
    "SIZE = 1000 # Maximum number of records to return per page ( default = 20 ).\n",
    "temp_params = { \"eventType\": events['temperature'],\n",
    "                #\"bbox\": bboxes['downtown_san_diego'],\n",
    "                #\"locationType\": locations['traffic'],\n",
    "                \"startTime\": STARTTS,\n",
    "                \"endTime\": ENDTS,\n",
    "                \"pageSize\": SIZE\n",
    "               }\n",
    "\n",
    "# Return event data by assetUid.\n",
    "temp_arr = data_by_assetUid(temp_meta_df[temp_meta_df['parentAssetUid'] == pauid].assetUid.values, temp_headers, temp_params)\n",
    "temp_arr = flatten(temp_arr) # Extract the inner list of dictionaries from the list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CityIQ Asset Media Data\n",
    "We currently do not have access to the Metrology or Media zones, so cannot access their data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CityIQ Media Service URL.\n",
    "# media_url = \"\"\n",
    "\n",
    "# MEDIA = \"\" # Media type ().\n",
    "# MTYPE = \"mediaType:\" + MEDIA # Filter response by media type. CAMERA is the only sensor that will generate mediaType."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis:\n",
    "The goal of Vision Zero is to reduce the count of traffic-related deaths to zero. Traffic-related deaths occur when there are collisions with certain properties between vehicles, between vehicles and pedestrians, or between vehicles and bicyclists, and are a subset of the possible results of a collision ( which also include non-/injurious collisions ). Collisions occur when objects intersect in time and space. We want to learn what properties lead to the collisions that result in traffic-related deaths. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asset Parser\n",
    "\n",
    "We need to parse each asset to extract all the data we wish to collect. Below is a list of the columns of interest:\n",
    "\n",
    "* assetUid\n",
    "* eventType\n",
    "* measures ( dicitionary-type; need to extract values from keys )\n",
    "* properties ( dicitionary-type; need to extract values from keys )\n",
    "* timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to DataFrames.\n",
    "pedevts_df = pd.DataFrame(pedevt_arr)\n",
    "tfevts_df = pd.DataFrame(tfevt_arr)\n",
    "humevts_df = pd.DataFrame(hum_arr)\n",
    "orievts_df = pd.DataFrame(ori_arr)\n",
    "presevts_df = pd.DataFrame(pres_arr)\n",
    "tempevts_df = pd.DataFrame(temp_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract event measures and properties from dictionaries, and append to DataFrames.\n",
    "pedevts_measures_df = pd.DataFrame(pedevts_df.measures.tolist())\n",
    "pedevts_properties_df = pd.DataFrame(pedevts_df.properties.tolist())\n",
    "pedevts_df.drop(['measures','properties'],axis=1,inplace=True)\n",
    "pedevts = pd.concat([pedevts_df, pedevts_measures_df, pedevts_properties_df],axis=1)\n",
    "\n",
    "tfevts_measures_df = pd.DataFrame(tfevts_df.measures.tolist())\n",
    "tfevts_properties_df = pd.DataFrame(tfevts_df.properties.tolist())\n",
    "tfevts_df.drop(['measures','properties'],axis=1,inplace=True)\n",
    "tfevts = pd.concat([tfevts_df, tfevts_measures_df, tfevts_properties_df],axis=1)\n",
    "\n",
    "humevts_measures_df = pd.DataFrame(humevts_df.measures.tolist())\n",
    "humevts_properties_df = pd.DataFrame(humevts_df.properties.tolist())\n",
    "humevts_df.drop(['measures','properties'],axis=1,inplace=True)\n",
    "humevts = pd.concat([humevts_df, humevts_measures_df, humevts_properties_df],axis=1)\n",
    "\n",
    "orievts_measures_df = pd.DataFrame(orievts_df.measures.tolist())\n",
    "orievts_properties_df = pd.DataFrame(orievts_df.properties.tolist())\n",
    "orievts_df.drop(['measures','properties'],axis=1,inplace=True)\n",
    "orievts = pd.concat([orievts_df, orievts_measures_df, orievts_properties_df],axis=1)\n",
    "\n",
    "presevts_measures_df = pd.DataFrame(presevts_df.measures.tolist())\n",
    "presevts_properties_df = pd.DataFrame(presevts_df.properties.tolist())\n",
    "presevts_df.drop(['measures','properties'],axis=1,inplace=True)\n",
    "presevts = pd.concat([presevts_df, presevts_measures_df, presevts_properties_df],axis=1)\n",
    "\n",
    "tempevts_measures_df = pd.DataFrame(tempevts_df.measures.tolist())\n",
    "tempevts_properties_df = pd.DataFrame(tempevts_df.properties.tolist())\n",
    "tempevts_df.drop(['measures','properties'],axis=1,inplace=True)\n",
    "tempevts = pd.concat([tempevts_df, tempevts_measures_df, tempevts_properties_df],axis=1)\n",
    "\n",
    "# Display DataFrames.\n",
    "display(pedevts.sample(3))\n",
    "display(tfevts.sample(3))\n",
    "display(humevts.sample(3))\n",
    "display(orievts.sample(3))\n",
    "display(presevts.sample(3))\n",
    "display(tempevts.sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print dimensions of event DataFrames (rows, columns).\n",
    "print(\"Pedestrian events: \", pedevts.shape)\n",
    "print(\"Traffic events: \", tfevts.shape)\n",
    "print(\"Humidity events: \", humevts.shape)\n",
    "print(\"Orientation events: \", orievts.shape)\n",
    "print(\"Pressure events: \", presevts.shape)\n",
    "print(\"Temperature events: \", tempevts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like for 5-1-2019, 89 pedestrian events, 2,144 traffic events, and 49 events of each environmental event type were captured by the assets on this node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rename the columns, so that we can append everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename event features. \n",
    "pedevts.columns = [\"pedevt_\" + cols for cols in list(pedevts)]\n",
    "tfevts.columns = [\"tfevt_\" + cols for cols in list(tfevts)]\n",
    "humevts.columns = [\"humevt_\" + cols for cols in list(humevts)]\n",
    "orievts.columns = [\"orievt_\" + cols for cols in list(orievts)]\n",
    "presevts.columns = [\"presevt_\" + cols for cols in list(presevts)]\n",
    "tempevts.columns = [\"tempevt_\" + cols for cols in list(tempevts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append all event DataFrames together ( tempevts data determined to be outside the scope of this project ).\n",
    "all_data_df = pedevts.append([tfevts, humevts, orievts, presevts], sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = './' \n",
    "# Example filename ( format: parentAssetUid_DATE.csv ): 08cbccff-cdd0-404f-af62-0346a4480d5c_2019-05-1.csv\n",
    "all_data_df.to_csv(os.path.join(filepath,r'08cbccff-cdd0-404f-af62-0346a4480d5c_2019-05-1.csv'),index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
